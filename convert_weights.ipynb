{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52da6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af7060e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt=torch.load('resnet18-f37072fd.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5458416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1adc0f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['bn1.running_mean'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bba2d2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['bn1.running_var'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c33376f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['bn1.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "29a787c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['bn1.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "81bbddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "f04a0e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyan/venv/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/lyan/venv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/lyan/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [00:04<00:00, 9.51MB/s]\n"
     ]
    }
   ],
   "source": [
    "m=resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "c0c99523",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=list(m.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "b51b1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_graph(model, output_name):\n",
    "    submodules=list(model.children())\n",
    "    \n",
    "    f = open(output_name, 'wb')\n",
    "    n=3\n",
    "    \n",
    "    header = np.zeros(256, dtype=np.int32)\n",
    "    header[0]=42\n",
    "    header[1]=1\n",
    "    header[2] = n\n",
    "\n",
    "    f.write(header.tobytes())\n",
    "    \n",
    "    for i in range(n):\n",
    "        l=submodules[i]\n",
    "            \n",
    "        if type(l) == torch.nn.Conv2d:\n",
    "            header = np.zeros(256, dtype=np.int32)\n",
    "            header[0]=1 # type for conv\n",
    "            header[1]=l.in_channels; header[2]=l.out_channels\n",
    "            header[3]=l.kernel_size[0]; header[4]=l.kernel_size[1]\n",
    "            header[5]=l.stride[0];header[6]=l.stride[1]\n",
    "            \n",
    "            header[7]=l.padding[0];header[8]=l.padding[1]\n",
    "            if l.bias is None:\n",
    "                header[9]=0\n",
    "            else:\n",
    "                header[9]=int(l.bias)\n",
    "            header[10]=l.groups\n",
    "            header[11]=l.dilation[0];header[12]=l.dilation[1];\n",
    "            \n",
    "            weight=l.weight.detach().to(torch.float32).numpy()\n",
    "            for i in range(len(weight.shape)):\n",
    "                header[13+i] = weight.shape[i]\n",
    "\n",
    "            f.write(header.tobytes())\n",
    "            f.write(weight.tobytes())\n",
    "            if l.bias is not None or l.bias:\n",
    "                f.write(l.bias.detach().to(torch.float32).numpy().tobytes())\n",
    "            \n",
    "        elif type(l) == torch.nn.BatchNorm2d:\n",
    "            header = np.zeros(256, dtype=np.int32)\n",
    "            header[0]=2 # type for conv\n",
    "            header[1] = l.num_features\n",
    "            \n",
    "            f.write(header.tobytes())\n",
    "            f.write(l.weight.detach().to(torch.float32).numpy().tobytes())\n",
    "            f.write(l.bias.detach().to(torch.float32).numpy().tobytes())\n",
    "            f.write(l.running_mean.detach().to(torch.float32).numpy().tobytes())\n",
    "            f.write(l.running_var.detach().to(torch.float32).numpy().tobytes())\n",
    "        elif type(l) == torch.nn.ReLU:\n",
    "            header = np.zeros(256, dtype=np.int32)\n",
    "            header[0]=3 # type for conv\n",
    "            f.write(header.tobytes())\n",
    "            \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "4aa0ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_graph(m, 'r18.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "258bd503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(x, name, output_name):\n",
    "    with open(output_name, 'wb') as f:\n",
    "        header = np.zeros(256, dtype=np.int32)\n",
    "        header[0]=42\n",
    "        header[1]=1\n",
    "        for i in range(len(x.shape)):\n",
    "            header[2+i] = x.shape[i]\n",
    "        \n",
    "        f.write(header.tobytes())\n",
    "        \n",
    "        f.write(x.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "871ad33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1025c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv2.imread('/home/lyan/Downloads/000000062491.jpg')\n",
    "# img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img=cv2.resize(img, (64,64))\n",
    "img=torch.tensor(img).reshape(1,3,64,64).to(torch.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a35afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f03c2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data(img.numpy(), '', 'test_img.bin')\n",
    "write_data(ckpt['conv1.weight'].detach().to(torch.float32).numpy(), 'conv1.weight', 'test.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8af3bc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=torch.nn.functional.conv2d(img, ckpt['conv1.weight'].detach() , )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "42377a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1196, -0.9304, -0.6296, -0.4332, -0.5840, -1.0241, -1.4404, -1.6431,\n",
       "        -1.7220, -1.7734, -1.8642, -1.9655, -1.9549, -1.8048, -1.5887, -1.3734,\n",
       "        -1.1993, -1.0883, -1.0240, -0.9661])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0,0].reshape(-1)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7a991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
